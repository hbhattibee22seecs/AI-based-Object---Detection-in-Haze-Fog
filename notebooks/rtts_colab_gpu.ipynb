{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09bce046",
   "metadata": {},
   "source": [
    "# RTTS Training & Jetson Deployment (Colab)\n",
    "This notebook automates RTTS dataset preparation, YOLOX-S training on a Colab GPU runtime, and packaging of artifacts for Jetson Nano inference. Run the cells sequentially after switching the Colab runtime to **GPU** (`Runtime -> Change runtime type -> GPU`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a61b602",
   "metadata": {},
   "source": [
    "## 1. Configure Colab GPU Environment\n",
    "The following cells verify GPU availability, install system dependencies, and clone this repository into `/content/Object-Detection-in-Hazy-and-Foggy-Conditions-on-NVIDIA-Jetson-Nano`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ae3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that Colab attached a GPU runtime\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589159a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository and install Python dependencies\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = \"https://github.com/wassihaiderkabir/Object-Detection-in-Hazy-and-Foggy-Conditions-on-NVIDIA-Jetson-Nano.git\"\n",
    "REPO_DIR = Path(\"/content/Object-Detection-in-Hazy-and-Foggy-Conditions-on-NVIDIA-Jetson-Nano\")\n",
    "if not REPO_DIR.exists():\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "\n",
    "%cd {REPO_DIR}\n",
    "!python -m pip install --upgrade pip wheel\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1232c0",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset\n",
    "This section downloads the RTTS subset of RESIDE into `/content/data/RTTS`, verifies the Pascal VOC-style splits, and runs the RTTS sanity checker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unpack RTTS into /content/data/RTTS\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "DATA_ROOT = Path(\"/content/data\")\n",
    "RTTS_DIR = DATA_ROOT / \"RTTS\"\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "if not RTTS_DIR.exists():\n",
    "    !wget -q https://residedata.obs.cn-north-4.myhuaweicloud.com/RTTS.zip -O {DATA_ROOT / \"RTTS.zip\"}\n",
    "    !unzip -q {DATA_ROOT / \"RTTS.zip\"} -d {DATA_ROOT}\n",
    "    !rm {DATA_ROOT / \"RTTS.zip\"}\n",
    "print(f\"RTTS dataset ready at {RTTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1312a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the RTTS sanity checker to confirm class distribution\n",
    "import os\n",
    "os.environ[\"YOLOX_DATADIR\"] = str(DATA_ROOT)\n",
    "%cd {REPO_DIR}\n",
    "!python src/Jetson_src/temp_inspect_rtts.py --refresh --limit 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b5b76",
   "metadata": {},
   "source": [
    "### Results Directory Setup\n",
    "Create a unified `/content/.../results` tree so plots, weights, and inference artifacts end up in one place for easy download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shared results directories and handy path helpers\n",
    "RESULTS_DIR = REPO_DIR / \"results\"\n",
    "PLOTS_DIR = RESULTS_DIR / \"plots\"\n",
    "WEIGHTS_DIR = RESULTS_DIR / \"weights\"\n",
    "INFER_DIR = RESULTS_DIR / \"inference_samples\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "for path in (PLOTS_DIR, WEIGHTS_DIR, INFER_DIR):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXP_NAME = \"rtts_yolox_s\"\n",
    "OUTPUT_DIR = REPO_DIR / \"YOLOX_outputs\" / EXP_NAME\n",
    "BEST_CKPT = OUTPUT_DIR / \"best_ckpt.pth\"\n",
    "LATEST_CKPT = OUTPUT_DIR / \"latest_ckpt.pth\"\n",
    "TENSORBOARD_DIR = OUTPUT_DIR / \"tensorboard\"\n",
    "print(f\"Results will be stored under {RESULTS_DIR}\")\n",
    "print(f\"TensorBoard logs expected in {TENSORBOARD_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cc62f3",
   "metadata": {},
   "source": [
    "## 3. Build Model Architecture\n",
    "YOLOX supplies the RTTS-specific experiment in `src/exps/example/custom/rtts_yolox_s.py`. The next cell prints key hyperparameters so you can tweak depth/width, input resolution, or augmentation knobs before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the core experiment settings for YOLOX-S on RTTS\n",
    "from pathlib import Path\n",
    "exp_path = REPO_DIR / \"src\" / \"exps\" / \"example\" / \"custom\" / \"rtts_yolox_s.py\"\n",
    "print(exp_path.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf841279",
   "metadata": {},
   "source": [
    "## 4. Train Model with GPU Acceleration\n",
    "We launch `src/Jetson_src/train_rtts.py`, which wires `YOLOX_DATADIR`, enables mixed precision, and saves checkpoints under `YOLOX_outputs/rtts_yolox_s/`. Adjust `--batch-size`, `--max-epochs`, or `--amp/--no-amp` to fit your GPU budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37abbe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch fine-tuning on RTTS\n",
    "%cd {REPO_DIR}\n",
    "!python src/Jetson_src/train_rtts.py \\\n",
    "    --colab \\\n",
    "    --datadir /content/data \\\n",
    "    --exp src/exps/example/custom/rtts_yolox_s.py \\\n",
    "    --ckpt src/yolox/weights/yolox_s.pth \\\n",
    "    --exp-name {EXP_NAME} \\\n",
    "    --batch-size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66558723",
   "metadata": {},
   "source": [
    "### Persist Checkpoints to `results/weights`\n",
    "Mirror the best and latest checkpoints into the shared results directory so they can be zipped or downloaded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy checkpoints into results/weights for convenient download\n",
    "import shutil\n",
    "weights_copied = 0\n",
    "for ckpt in [BEST_CKPT, LATEST_CKPT]:\n",
    "    if ckpt.exists():\n",
    "        destination = WEIGHTS_DIR / ckpt.name\n",
    "        shutil.copy2(ckpt, destination)\n",
    "        weights_copied += 1\n",
    "        print(f\"Copied {ckpt.name} -> {destination}\")\n",
    "    else:\n",
    "        print(f\"[WARN] {ckpt} not found yet; re-run after training finishes.\")\n",
    "print(f\"Total checkpoints mirrored: {weights_copied}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0436d1",
   "metadata": {},
   "source": [
    "## 5. Evaluate and Save Artifacts\n",
    "After training completes, run evaluation on the validation split, export ONNX plus TensorRT-ready assets, and bundle everything for download to your workstation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e17f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best checkpoint on RTTS val split\n",
    "!python src/tools/eval.py \\\n",
    "    -f src/exps/example/custom/rtts_yolox_s.py \\\n",
    "    -c {BEST_CKPT} \\\n",
    "    -b 1 -d 1 --conf 0.001 --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export ONNX and convert to TensorRT-friendly artifacts\n",
    "EXPORT_DIR = REPO_DIR / \"exports\"\n",
    "EXPORT_DIR.mkdir(exist_ok=True)\n",
    "ONNX_PATH = EXPORT_DIR / \"rtts_yolox_s.onnx\"\n",
    "!python src/tools/export_onnx.py \\\n",
    "    -f src/exps/example/custom/rtts_yolox_s.py \\\n",
    "    -c {BEST_CKPT} \\\n",
    "    --output-file {ONNX_PATH} \\\n",
    "    --input [640,640]\n",
    "\n",
    "TRT_ENGINE = EXPORT_DIR / \"rtts_yolox_s_fp16.trt\"\n",
    "!python src/tools/trt.py \\\n",
    "    -f src/exps/example/custom/rtts_yolox_s.py \\\n",
    "    -c {BEST_CKPT} \\\n",
    "    --trt_fp16 --device 0 \\\n",
    "    --output-name {TRT_ENGINE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f18f99",
   "metadata": {},
   "source": [
    "### Visualize Training & Validation Curves\n",
    "Use the TensorBoard event logs to chart training losses, learning rate, and validation AP. All plots are saved under `results/plots`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training/validation curves from TensorBoard logs\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "\n",
    "if not TENSORBOARD_DIR.exists():\n",
    "    raise FileNotFoundError(f\"TensorBoard directory not found: {TENSORBOARD_DIR}\")\n",
    "\n",
    "accumulator = EventAccumulator(str(TENSORBOARD_DIR))\n",
    "accumulator.Reload()\n",
    "scalar_tags = set(accumulator.Tags().get(\"scalars\", []))\n",
    "\n",
    "\n",
    "def load_series(tag: str) -> pd.DataFrame:\n",
    "    events = accumulator.Scalars(tag)\n",
    "    return pd.DataFrame({\n",
    "        \"step\": [event.step for event in events],\n",
    "        \"value\": [event.value for event in events],\n",
    "        \"tag\": tag,\n",
    "    })\n",
    "\n",
    "# Training losses\n",
    "train_tags = [\n",
    "    tag for tag in (\n",
    "        \"train/total_loss\",\n",
    "        \"train/iou_loss\",\n",
    "        \"train/conf_loss\",\n",
    "        \"train/cls_loss\",\n",
    "        \"train/lr\",\n",
    "    ) if tag in scalar_tags\n",
    "]\n",
    "\n",
    "figures = []\n",
    "if train_tags:\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    for tag in train_tags:\n",
    "        series = load_series(tag)\n",
    "        label = tag.split(\"/\", 1)[1]\n",
    "        ax.plot(series[\"step\"], series[\"value\"], label=label)\n",
    "    ax.set_xlabel(\"iteration\")\n",
    "    ax.set_ylabel(\"value\")\n",
    "    ax.set_title(\"Training losses & LR\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    fig.tight_layout()\n",
    "    train_plot = PLOTS_DIR / \"train_curves.png\"\n",
    "    fig.savefig(train_plot, dpi=200)\n",
    "    figures.append(train_plot)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No train/* scalars in TensorBoard logs (did training finish?).\")\n",
    "\n",
    "# Validation AP curves\n",
    "val_tags = [tag for tag in (\"val/COCOAP50\", \"val/COCOAP50_95\") if tag in scalar_tags]\n",
    "if val_tags:\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    for tag in val_tags:\n",
    "        series = load_series(tag)\n",
    "        label = tag.split(\"/\", 1)[1]\n",
    "        ax.plot(series[\"step\"], series[\"value\"], marker=\"o\", label=label)\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"AP\")\n",
    "    ax.set_title(\"Validation AP progression\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    fig.tight_layout()\n",
    "    val_plot = PLOTS_DIR / \"val_ap.png\"\n",
    "    fig.savefig(val_plot, dpi=200)\n",
    "    figures.append(val_plot)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No val/* scalars found; ensure eval_interval > 0 during training.\")\n",
    "\n",
    "print(\"Stored plots:\")\n",
    "for fig_path in figures:\n",
    "    print(\" -\", fig_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb22918",
   "metadata": {},
   "source": [
    "### Local Inference Smoke Test\n",
    "Run `tools/demo.py` on a representative RTTS image, copy the rendered result into `results/inference_samples`, and display it inline to verify the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a quick image inference and archive the rendered output\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from IPython.display import Image as IPyImage, display\n",
    "\n",
    "jpeg_root = RTTS_DIR / \"JPEGImages\"\n",
    "sample_candidates = sorted(list(jpeg_root.glob(\"*.png\"))) or sorted(list(jpeg_root.glob(\"*.jpg\")))\n",
    "if not sample_candidates:\n",
    "    raise FileNotFoundError(f\"No images found inside {jpeg_root}\")\n",
    "SAMPLE_IMAGE = sample_candidates[0]\n",
    "print(f\"Using sample image: {SAMPLE_IMAGE}\")\n",
    "\n",
    "vis_root = OUTPUT_DIR / \"vis_res\"\n",
    "!python src/tools/demo.py image \\\n",
    "    -f src/exps/example/custom/rtts_yolox_s.py \\\n",
    "    -c {BEST_CKPT} \\\n",
    "    --path {SAMPLE_IMAGE} \\\n",
    "    --conf 0.25 --nms 0.45 \\\n",
    "    --device gpu --fp16 --save_result\n",
    "\n",
    "if not vis_root.exists():\n",
    "    raise FileNotFoundError(f\"No visualization directory found at {vis_root}\")\n",
    "latest_folder = max(vis_root.iterdir(), key=lambda d: d.stat().st_mtime)\n",
    "rendered = list(latest_folder.glob(SAMPLE_IMAGE.name)) or list(latest_folder.glob(\"*\"))\n",
    "if not rendered:\n",
    "    raise FileNotFoundError(f\"No rendered files found under {latest_folder}\")\n",
    "render_path = rendered[0]\n",
    "target = INFER_DIR / render_path.name\n",
    "shutil.copy2(render_path, target)\n",
    "print(f\"Copied rendered image to {target}\")\n",
    "display(IPyImage(filename=target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c38050",
   "metadata": {},
   "source": [
    "### Confusion Matrix on Validation Split\n",
    "Re-run evaluation inside this notebook, collect detections, and derive a confusion matrix (IoU â‰¥ 0.5) to spot which classes still confuse the model. Results are written to `results/plots/confusion_matrix.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed742f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a confusion matrix by matching predictions to ground truth at IoU >= 0.5\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from yolox.exp import get_exp\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "exp = get_exp(str(REPO_DIR / \"src\" / \"exps\" / \"example\" / \"custom\" / \"rtts_yolox_s.py\"), None)\n",
    "model = exp.get_model()\n",
    "ckpt = torch.load(BEST_CKPT, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "model.to(device).eval()\n",
    "\n",
    "evaluator = exp.get_evaluator(batch_size=1, is_distributed=False)\n",
    "(_, _, _), predictions = evaluator.evaluate(model, half=torch.cuda.is_available(), return_outputs=True)\n",
    "dataset = evaluator.dataloader.dataset\n",
    "class_names = list(getattr(dataset, \"_classes\", [f\"class_{i}\" for i in range(exp.num_classes)]))\n",
    "labels = class_names + [\"background\"]\n",
    "mat = np.zeros((len(labels), len(labels)), dtype=int)\n",
    "iou_thresh = 0.5\n",
    "max_images = min(len(predictions), 400)\n",
    "\n",
    "\n",
    "def iou(box, boxes):\n",
    "    if boxes.size == 0:\n",
    "        return np.array([])\n",
    "    box = np.expand_dims(box, axis=0)\n",
    "    lt = np.maximum(box[..., :2], boxes[..., :2])\n",
    "    rb = np.minimum(box[..., 2:], boxes[..., 2:])\n",
    "    wh = np.clip(rb - lt, a_min=0, a_max=None)\n",
    "    inter = wh[..., 0] * wh[..., 1]\n",
    "    box_area = (box[..., 2] - box[..., 0]) * (box[..., 3] - box[..., 1])\n",
    "    boxes_area = (boxes[..., 2] - boxes[..., 0]) * (boxes[..., 3] - boxes[..., 1])\n",
    "    union = box_area + boxes_area - inter\n",
    "    return inter / np.clip(union, a_min=1e-6, a_max=None)\n",
    "\n",
    "\n",
    "def to_numpy(pred_tuple):\n",
    "    if pred_tuple[0] is None:\n",
    "        return np.zeros((0, 4)), np.zeros((0,), dtype=int), np.zeros((0,))\n",
    "    boxes, cls, scores = pred_tuple\n",
    "    return boxes.numpy(), cls.numpy().astype(int), scores.numpy()\n",
    "\n",
    "\n",
    "processed = 0\n",
    "for img_id in sorted(predictions.keys()):\n",
    "    if processed >= max_images:\n",
    "        break\n",
    "    pred_boxes, pred_cls, pred_scores = to_numpy(predictions[img_id])\n",
    "    gt = dataset.annotations[img_id][0] if len(dataset.annotations) > img_id else np.zeros((0, 5))\n",
    "    gt_boxes = gt[:, :4] if gt.size else np.zeros((0, 4))\n",
    "    gt_cls = gt[:, 4].astype(int) if gt.size else np.zeros((0,), dtype=int)\n",
    "\n",
    "    order = np.argsort(-pred_scores)\n",
    "    pred_boxes, pred_cls, pred_scores = pred_boxes[order], pred_cls[order], pred_scores[order]\n",
    "    matched_gt = set()\n",
    "\n",
    "    for p_box, p_cls in zip(pred_boxes, pred_cls):\n",
    "        if gt_boxes.size:\n",
    "            ious = iou(p_box, gt_boxes)\n",
    "            best_idx = int(np.argmax(ious)) if ious.size else -1\n",
    "            best_iou = ious[best_idx] if ious.size else 0.0\n",
    "        else:\n",
    "            best_idx, best_iou = -1, 0.0\n",
    "\n",
    "        if best_idx >= 0 and best_iou >= iou_thresh and best_idx not in matched_gt:\n",
    "            gt_label = gt_cls[best_idx]\n",
    "            mat[gt_label, p_cls] += 1\n",
    "            matched_gt.add(best_idx)\n",
    "        else:\n",
    "            mat[len(class_names), p_cls] += 1  # false positives counted in last row\n",
    "\n",
    "    for gt_idx, gt_label in enumerate(gt_cls):\n",
    "        if gt_idx not in matched_gt:\n",
    "            mat[gt_label, len(class_names)] += 1  # misses counted in last column\n",
    "\n",
    "    processed += 1\n",
    "\n",
    "print(f\"Confusion stats built from {processed} validation images\")\n",
    "cm_path = PLOTS_DIR / \"confusion_matrix.png\"\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(mat, cmap=\"Blues\")\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel(\"Predicted class\")\n",
    "ax.set_ylabel(\"Ground truth class\")\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "fig.tight_layout()\n",
    "fig.savefig(cm_path, dpi=220)\n",
    "plt.show()\n",
    "print(f\"Confusion matrix saved to {cm_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9176e5",
   "metadata": {},
   "source": [
    "## 6. Package Jetson Nano Inference Script\n",
    "Jetson-friendly inference uses TensorRT plus lightweight preprocessing. The cell below emits `src/Jetson_src/jetson_nano_runner.py`, which loads the exported ONNX/TensorRT engine, runs warmup, and exposes CLI switches for images, video files, or live cameras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Jetson Nano inference helper script\n",
    "from textwrap import dedent\n",
    "\n",
    "script_path = REPO_DIR / \"src\" / \"Jetson_src\" / \"jetson_nano_runner.py\"\n",
    "script_path.write_text(dedent('''#!/usr/bin/env python3\n",
    "\"\"\"TensorRT-friendly inference helper tailored for Jetson Nano.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[2]\n",
    "DEFAULT_EXP = ROOT / \"src\" / \"exps\" / \"example\" / \"custom\" / \"rtts_yolox_s.py\"\n",
    "DEFAULT_EXP_NAME = \"rtts_yolox_s\"\n",
    "DEFAULT_CKPT = ROOT / \"YOLOX_outputs\" / DEFAULT_EXP_NAME / \"best_ckpt.pth\"\n",
    "DEFAULT_TRT = ROOT / \"YOLOX_outputs\" / DEFAULT_EXP_NAME / \"model_trt.pth\"\n",
    "DEMO_SCRIPT = ROOT / \"src\" / \"tools\" / \"demo.py\"\n",
    "\n",
    "\n",
    "def _run(cmd: List[str]) -> None:\n",
    "    print(\"[jetson_nano_runner]\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True, cwd=ROOT)\n",
    "\n",
    "\n",
    "def _ensure_trt(args: argparse.Namespace) -> None:\n",
    "    if args.trt_file.exists() and not args.rebuild:\n",
    "        print(f\"[jetson_nano_runner] Reusing TensorRT weights at {args.trt_file}\")\n",
    "        return\n",
    "\n",
    "    build_cmd = [\n",
    "        \"python3\",\n",
    "        \"src/tools/trt.py\",\n",
    "        \"-f\",\n",
    "        str(args.exp),\n",
    "        \"-c\",\n",
    "        str(args.ckpt),\n",
    "        \"-expn\",\n",
    "        args.exp_name,\n",
    "    ]\n",
    "    _run(build_cmd)\n",
    "\n",
    "    built_file = ROOT / \"YOLOX_outputs\" / args.exp_name / \"model_trt.pth\"\n",
    "    if not built_file.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"TensorRT conversion did not produce {built_file}. Check tools/trt.py logs.\"\n",
    "        )\n",
    "    args.trt_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(built_file, args.trt_file)\n",
    "    print(f\"[jetson_nano_runner] Copied TensorRT weights to {args.trt_file}\")\n",
    "\n",
    "\n",
    "def _launch_demo(args: argparse.Namespace) -> None:\n",
    "    cmd = [\n",
    "        \"python3\",\n",
    "        str(DEMO_SCRIPT),\n",
    "        args.mode,\n",
    "        \"-f\",\n",
    "        str(args.exp),\n",
    "        \"--device\",\n",
    "        \"gpu\",\n",
    "        \"--conf\",\n",
    "        str(args.conf),\n",
    "        \"--nms\",\n",
    "        str(args.nms),\n",
    "        \"--trt\",\n",
    "        \"--trt-file\",\n",
    "        str(args.trt_file),\n",
    "        \"--save_result\",\n",
    "    ]\n",
    "    if args.fp16:\n",
    "        cmd.append(\"--fp16\")\n",
    "    if args.tsize:\n",
    "        cmd.extend([\"--tsize\", str(args.tsize)])\n",
    "\n",
    "    if args.mode in {\"image\", \"video\"}:\n",
    "        cmd.extend([\"--path\", str(args.input)])\n",
    "    else:\n",
    "        cmd.extend([\"--camid\", str(args.cam_id)])\n",
    "\n",
    "    _run(cmd)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser(description=__doc__)\n",
    "    parser.add_argument(\"--exp\", type=Path, default=DEFAULT_EXP, help=\"Experiment file\")\n",
    "    parser.add_argument(\"--exp-name\", default=DEFAULT_EXP_NAME, help=\"Experiment/output name\")\n",
    "    parser.add_argument(\"--ckpt\", type=Path, default=DEFAULT_CKPT, help=\"Checkpoint path\")\n",
    "    parser.add_argument(\"--trt-file\", type=Path, default=DEFAULT_TRT, help=\"TensorRT weight file\")\n",
    "    parser.add_argument(\"--mode\", choices=[\"image\", \"video\", \"webcam\"], default=\"image\")\n",
    "    parser.add_argument(\"--input\", type=Path,\n",
    "                        default=ROOT / \"data\" / \"RTTS\" / \"JPEGImages\" / \"AM_Bing_211.png\")\n",
    "    parser.add_argument(\"--cam-id\", type=int, default=0)\n",
    "    parser.add_argument(\"--conf\", type=float, default=0.25)\n",
    "    parser.add_argument(\"--nms\", type=float, default=0.45)\n",
    "    parser.add_argument(\"--tsize\", type=int, default=640, help=\"Square inference size\")\n",
    "    parser.add_argument(\"--datadir\", type=Path, default=ROOT / \"data\")\n",
    "    parser.add_argument(\"--rebuild\", action=\"store_true\", help=\"Force TensorRT rebuild\")\n",
    "    parser.add_argument(\"--skip-build\", action=\"store_true\", help=\"Skip TensorRT conversion\")\n",
    "    parser.add_argument(\"--fp16\", action=\"store_true\", help=\"Enable FP16 inference\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.environ.setdefault(\"YOLOX_DATADIR\", str(args.datadir.resolve()))\n",
    "\n",
    "    if not args.skip_build:\n",
    "        _ensure_trt(args)\n",
    "\n",
    "    _launch_demo(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''))\n",
    "script_path.chmod(0o755)\n",
    "print(f\"Jetson Nano runner created at {script_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45083ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bundle key artifacts (results, exports, scripts) into a single zip for download\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "BUNDLE_NAME = \"rtts_yolox_s_artifacts.zip\"\n",
    "BUNDLE_PATH = RESULTS_DIR / BUNDLE_NAME\n",
    "if BUNDLE_PATH.exists():\n",
    "    BUNDLE_PATH.unlink()\n",
    "\n",
    "paths_to_package = [\n",
    "    RESULTS_DIR,\n",
    "    EXPORT_DIR,\n",
    "    REPO_DIR / \"src\" / \"exps\" / \"example\" / \"custom\" / \"rtts_yolox_s.py\",\n",
    "    REPO_DIR / \"src\" / \"Jetson_src\" / \"jetson_nano_runner.py\",\n",
    "]\n",
    "\n",
    "files_to_add = []\n",
    "for path in paths_to_package:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        print(f\"[WARN] Skipping missing path: {path}\")\n",
    "        continue\n",
    "    if path.is_dir():\n",
    "        files_to_add.extend([p for p in path.rglob(\"*\") if p.is_file()])\n",
    "    else:\n",
    "        files_to_add.append(path)\n",
    "\n",
    "with zipfile.ZipFile(BUNDLE_PATH, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "    for file_path in files_to_add:\n",
    "        if file_path.resolve() == BUNDLE_PATH.resolve():\n",
    "            continue\n",
    "        arcname = file_path.relative_to(REPO_DIR)\n",
    "        zf.write(file_path, arcname)\n",
    "\n",
    "print(f\"Bundle ready at {BUNDLE_PATH} (contains {len(files_to_add)} files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d38419",
   "metadata": {},
   "source": [
    "## 7. Document Jetson Nano Setup & Deployment Steps\n",
    "1. **Flash & update Jetson Nano**: Install JetPack 5.1+ (ships with CUDA, cuDNN, TensorRT). Run `sudo nvpmodel -m 0 && sudo jetson_clocks` to unlock the 10 W power profile.\n",
    "2. **Install dependencies** (already bundled with JetPack, just add extras): `sudo apt install python3-pip libopencv-dev && pip3 install --upgrade pip` followed by `pip3 install -r requirements.txt --extra-index-url https://pypi.ngc.nvidia.com` inside the repo directory.\n",
    "3. **Copy artifacts** produced by this notebook: `scp rtts_yolox_s_artifacts.zip jetson@nano.local:~/haze-yolox/ && unzip rtts_yolox_s_artifacts.zip`.\n",
    "4. **Verify datasets**: Place RTTS under `~/haze-yolox/data/RTTS` (same layout as on Colab) and set `YOLOX_DATADIR=/home/jetson/haze-yolox/data`.\n",
    "5. **Optional TensorRT rebuild**: `python3 src/tools/trt.py -f src/exps/example/custom/rtts_yolox_s.py -c YOLOX_outputs/rtts_yolox_s/best_ckpt.pth --trt_fp16 --output-name exports/rtts_yolox_s_fp16.trt`.\n",
    "6. **Run inference** with the helper script: `python3 src/Jetson_src/jetson_nano_runner.py --mode image --input data/RTTS/JPEGImages/AM_Bing_211.png --conf 0.25 --nms 0.45`. Switch to `--mode video --input foggy.mp4` or `--mode webcam --cam-id 0` as needed.\n",
    "7. **Monitor performance**: Use `tegrastats` to watch GPU load/temperature and adjust resolution via `--img-size 512 512` if FPS dips below your target."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
